const axios = require('axios');
const config = require('../config');
const logger = require('../utils/logger');
const fs = require('fs');
const path = require('path');
const { getAxiosProxyConfig } = require('../utils/proxyUtils');

const vectorMemory = require('../services/vectorMemoryService');

class AiHandler {
    constructor() {
        this.contexts = new Map(); // groupId -> [{role, content}, ...]
        this.dataDir = path.join(process.cwd(), 'data');
        this.contextsDir = path.join(this.dataDir, 'contexts');
        this.legacyFile = path.join(this.dataDir, 'ai_contexts.json');
        this.saveTimers = new Map(); // groupId -> timer
        this.init();
    }

    // Initialize storage and migrate legacy data if exists
    init() {
        try {
            if (!fs.existsSync(this.dataDir)) {
                fs.mkdirSync(this.dataDir, { recursive: true });
            }
            if (!fs.existsSync(this.contextsDir)) {
                fs.mkdirSync(this.contextsDir, { recursive: true });
            }

            // Check for legacy file and migrate
            if (fs.existsSync(this.legacyFile)) {
                logger.info('[AiHandler] Found legacy chat history. Migrating...');
                const data = fs.readFileSync(this.legacyFile, 'utf8');
                try {
                    const entries = JSON.parse(data);
                    // entries is [[key, value], ...]
                    for (const [key, value] of entries) {
                        const filePath = path.join(this.contextsDir, `${key}.json`);
                        fs.writeFileSync(filePath, JSON.stringify(value, null, 2), 'utf8');
                    }
                    // Rename legacy file to .bak
                    fs.renameSync(this.legacyFile, this.legacyFile + '.bak');
                    logger.info(`[AiHandler] Migrated ${entries.length} group histories to separate files.`);
                } catch (parseError) {
                    logger.error('[AiHandler] Failed to parse legacy history during migration:', parseError);
                }
            }
        } catch (e) {
            logger.error('[AiHandler] Failed to initialize storage:', e);
        }
    }

    // Get context for a group, loading from disk if necessary
    getContext(groupId) {
        if (this.contexts.has(groupId)) {
            return this.contexts.get(groupId);
        }

        const filePath = path.join(this.contextsDir, `${groupId}.json`);
        try {
            if (fs.existsSync(filePath)) {
                const data = fs.readFileSync(filePath, 'utf8');
                if (!data || data.trim() === '') {
                    throw new Error('Empty file');
                }
                const context = JSON.parse(data);
                this.contexts.set(groupId, context);
                return context;
            }
        } catch (e) {
            logger.error(`[AiHandler] Failed to load history for group ${groupId}:`, e);
        }

        // Return empty context if file doesn't exist or error
        const newContext = [];
        this.contexts.set(groupId, newContext);
        return newContext;
    }

    // Save context for a specific group asynchronously with debounce
    saveContext(groupId) {
        if (this.saveTimers.has(groupId)) {
            clearTimeout(this.saveTimers.get(groupId));
        }

        const timer = setTimeout(() => {
            try {
                const context = this.contexts.get(groupId);
                if (!context) return;

                if (!fs.existsSync(this.contextsDir)) {
                    fs.mkdirSync(this.contextsDir, { recursive: true });
                }
                
                const filePath = path.join(this.contextsDir, `${groupId}.json`);
                const data = JSON.stringify(context, null, 2);
                
                fs.writeFile(filePath, data, 'utf8', (err) => {
                    if (err) {
                        logger.error(`[AiHandler] Failed to save history for group ${groupId}:`, err);
                    }
                });
                
                this.saveTimers.delete(groupId);
            } catch (e) {
                logger.error(`[AiHandler] Error preparing to save history for group ${groupId}:`, e);
            }
        }, 1000); // Wait 1s after last change before saving

        this.saveTimers.set(groupId, timer);
    }

    async getReply(message, userId, groupId) {
        try {
            if (!config.aiApiKey) {
                logger.warn('AI_API_KEY is not set. Skipping AI reply.');
                return null;
            }

            // Initialize context for group if not exists
            const contextKey = groupId || userId;
            const fullContext = this.getContext(contextKey);

            // Limit context for API based on aiContextLimit
            const contextLimit = config.getGroupConfig(groupId, 'aiContextLimit');
            const context = fullContext.slice(-contextLimit);

            // RAG: Retrieve relevant long-term memories
            let systemPrompt = config.aiSystemPrompt;
            try {
                const relevantMemories = await vectorMemory.search(contextKey, message);
                if (relevantMemories.length > 0) {
                    const memoryText = relevantMemories.map(m => 
                        `[${new Date(m.timestamp).toLocaleString()}] ${m.role === 'user' ? 'User' : 'Assistant'}: ${m.text}`
                    ).join('\n');
                    systemPrompt += `\n\n【Related History/Memory】:\n${memoryText}\n【End of Memory】\n(Use these memories to maintain context consistency)`;
                    logger.info(`[AiHandler] Injected ${relevantMemories.length} relevant memories for group ${groupId}`);
                }
            } catch (err) {
                logger.error('[AiHandler] Vector search failed:', err);
            }
            
            // Construct messages array for API
            // System prompt is currently static from config
            // Use .map to strip userId from the context sent to OpenAI
            // Format content as "[User <id>]: <content>" so AI knows who said what
            // And clean content to avoid "I can't see QQ" issues
            const apiContext = context.map(msg => {
                let content = msg.content;
                // Clean CQ codes for AI consumption
                if (content) {
                    // Replace [CQ:at,qq=123] with @User123
                    content = content.replace(/\[CQ:at,qq=(\d+)\]/g, ' @User$1 ');
                    // Replace [CQ:image,...] with [图片]
                    content = content.replace(/\[CQ:image,[^\]]+\]/g, ' [图片] ');
                    // Remove other CQ codes to avoid confusion
                    content = content.replace(/\[CQ:[^\]]+\]/g, '');
                    content = content.trim();
                }

                // Format timestamp if available
                let timePrefix = '';
                if (msg.timestamp) {
                    const date = new Date(msg.timestamp);
                    timePrefix = `[${date.toLocaleString()}] `;
                }

                if (msg.role === 'user' && msg.userId) {
                    return { role: 'user', content: `${timePrefix}[用户 ${msg.userId}]: ${content}` };
                }
                return { role: msg.role, content: `${timePrefix}${content}` };
            });
            
            const messages = [
                { role: 'system', content: systemPrompt },
                ...apiContext
            ];

            const proxyConfig = getAxiosProxyConfig(config.aiChatProxy);
            const response = await axios.post(config.aiApiUrl, {
                model: config.aiModel,
                messages: messages
            }, {
                headers: {
                    'Authorization': `Bearer ${config.aiApiKey}`,
                    'Content-Type': 'application/json'
                },
                proxy: proxyConfig,
                timeout: 30000 // 30s timeout
            });

            if (response.data && response.data.choices && response.data.choices.length > 0) {
                const reply = response.data.choices[0].message.content.trim();
                
                // Add assistant reply to context (assistant has no userId)
                this.addMessageToContext(contextKey, 'assistant', reply);
                
                // Add to Vector Memory (Async)
                // Use cleaned content for vector embedding to improve quality
                // But we don't have cleaned user message easily accessible here except by re-cleaning
                // Let's just use raw message for now, vector models are usually robust enough
                // Or better, let's clean it before sending to vector memory
                
                // Clean user message for vector memory
                let cleanUserMsg = message;
                if (cleanUserMsg) {
                     cleanUserMsg = cleanUserMsg.replace(/\[CQ:at,qq=(\d+)\]/g, ' @User$1 ');
                     cleanUserMsg = cleanUserMsg.replace(/\[CQ:image,[^\]]+\]/g, ' [图片] ');
                     cleanUserMsg = cleanUserMsg.replace(/\[CQ:[^\]]+\]/g, '');
                     cleanUserMsg = cleanUserMsg.trim();
                }
                
                vectorMemory.addMemory(contextKey, cleanUserMsg, 'user');
                vectorMemory.addMemory(contextKey, reply, 'assistant');

                return reply;
            }
            
            logger.error('Unexpected AI API response structure:', response.data);
            return null;
        } catch (error) {
            if (error.response) {
                logger.error(`AI API Error (Status ${error.response.status}):`, error.response.data);
            } else {
                logger.error('AI API Request Error:', error.message);
            }
            return null;
        }
    }

    shouldReply(message, isAt, groupId) {
        if (isAt) return true;
        // Check if AI is enabled for this group (if there's a switch, but currently it's probability)
        // User mentioned "AI context menu" -> "ai上下文菜单" switch?
        // If user wants a switch, we might need a boolean config like 'aiEnabled'
        // But for now, user said "AI context menu" which might be "aiContextLimit".
        // Let's stick to probability for now or check if there's a new requirement.
        // But user said "AI context menu... adjusted to follow group ID".
        // If it means "Probability", I should use getGroupConfig for probability too?
        // User didn't mention probability explicitly, but "AI context menu" usually implies the feature itself.
        // I'll leave probability global unless user asked, but I will pass groupId to be safe.
        return Math.random() < config.aiProbability;
    }
    
    // Helper to add message, trim context, and trigger save
    addMessageToContext(groupId, role, content, userId = null) {
        const context = this.getContext(groupId);
        
        // Construct message object
        const msgObj = { 
            role, 
            content,
            timestamp: Date.now()
        };
        if (userId) {
            msgObj.userId = userId;
        }
        
        context.push(msgObj);
        
        // Trim context based on persistence limit (200)
        const persistenceLimit = 200;
        while (context.length > persistenceLimit) {
            context.shift();
        }
        
        // Trigger async save for this group
        this.saveContext(groupId);
    }

    // Reset context for a group
    resetContext(groupId) {
        this.contexts.set(groupId, []);
        this.saveContext(groupId);
        logger.info(`[AiHandler] Reset context for group ${groupId}`);
    }


}

module.exports = new AiHandler();
